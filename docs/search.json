[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Final Portfolio",
    "section": "",
    "text": "A short blog that contains my 5 mini-projects from the course, as well as a reflection that highlights how the mini-projects tie together and shows my biggest take-aways from each project."
  },
  {
    "objectID": "posts/05-hypothesis-testing/index.html",
    "href": "posts/05-hypothesis-testing/index.html",
    "title": "Hypothesis Testing",
    "section": "",
    "text": "Mini Project 5: Advantages and Drawbacks of Using p-values\nRead the editorial Moving to a World Beyond ‘p &lt; 0.05’ by Wasserstein, Schirm, and Lazar published in The American Statistician in 2019 as the introduction to a special issue about p-values. Answer a series of questions about it.\nStatement of Integrity: “All work presented is my own, and I have followed all rules for collaboration. I have not used generative AI on this project.”\nNorah Kuduk\n\nQuestions:\nTowards the end of Section 1, the authors say “As ‘statistical significance’ is used less, statistical thinking will be used more.” Elaborate on what you think the authors mean. Give some examples of what you think embodies “statistical thinking.”\nI think the authors mean that as a whole, the perspective on evaluating results needs to move away from threshold based thinking, and towards a more open approach. Statistical thinking highlights the idea that researchers will be able to communicate all of their findings, including those that might not be “accepted” under current standards today. It will validate results being “uncertain,” and focus on effective communication of the entirety of the analysis, and not just a single number that may or may not have value.\nSection 2, third paragraph: The authors state “A label of statistical significance adds nothing to what is already conveyed by the value of p; in fact, this dichotomization of p-values makes matters worse.” Elaborate on what you think the authors means.\nThe authors mean that innately, the definition of a p-value has nothing to do with statistical significance, and using some threshold to define whether something is statistically significant doesn’t make sense. The dichotomization of a p-value causes there to be a definite difference between results where there might not actually be much difference at all. For example, a p-value of 0.049 would be considered statistically significant, while a p-value of 0.051 is statistically insignificant.\nSection 2, end of first column: The authors state “For the integrity of scientific publishing and research dissemination, therefore, whether a p-value passes any arbitrary threshold should not be considered at all when deciding which results to present or highlight.” Do you agree or disagree? How should it be decided which results to present/highlight in scientific publishing?\nI agree that whether or not a specific a p-value passes any arbitrary threshold should not be considered when deciding what results to publish or not. It goes into the idea of exploratory vs. pre-planned studies, but it is also true that a result that is not statistically significant is just important to others than one that is. It is a core part of a intro stat course that a p-value can only give amounts of evidence to support a hypothesis, not give any proof about the association or effect of a variable, so it makes sense that one value or another shouldn’t be used to decide on presenting or withholding information accordingly.\nSection 3, end of page 2: The authors state “The statistical community has not yet converged on a simple paradigm for the use of statistical inference in scientific research – and in fact it may never do so. A one-size-fits-all approach to statistical inference is an inappropriate expectation.” Do you agree or disagree? Explain.\nI agree, “statistics” involves such a broad range of studies and techniques, that it honestly makes sense that there is not one single method that works for ever single statistical analysis. I do agree that for entry-level statistics, it makes sense to have something like the p-value threshold to introduce the ideas of interpreting results in a way that is more formulaic, but I think that it can be made more clear that while this is appropriate for intro level classes, as a whole the statistics community is moving away from the idea.\nSection 3.2: The authors note that they are envisioning “a sort of ‘statistical thoughtfulness’.” What do you think “statistical thoughtfulness” means? What are some ways to demonstrate “statistical thoughtfulness” in an analysis?\n“Statistical thoughtfulness” indicates a reflective approach to an analysis, and being open to the idea that a result may or may not have a clear importance to the study. Ways to be statistically thoughtful in an analysis involve defining the objectives of the research and clarifying what researchers want to see from the analysis. It also involves using multiple techniques or approaches to test practicality rather than relying on metrics. It also involves recognizing when in an analysis there were limitations to the techniques or when more subjective choices were made.\nSection 3.2.4: A few of the authors of papers in this special issue argue that some of the terminology used in statistics, such as “significance” and “confidence” can be misleading, and they propose the use of “compatibility” instead. What you do you think they believe the problem is? Do you agree or disagree (that there is a problem and that changing the name will help)?\nThe author’s believe that terms “significance” and “confidence” are misleading, which forces users into overconfident claims about their data. I agree that significance and confidence are easily misinterpreted, especially when learning statistical techniques for the first time. It reminds me of recently when we went over the difference between the frequentist and Bayesian methods, highlighting how in the frequentist interpretation, 95% confidence is often misinterpreted as 95% probability. Changing the name might help relieve some of that confusion.\nFind a quote or point that really strikes you (i.e., made you think). What is the quote (and tell me where to find it), and why does it stand out to you?\nSection 5, second paragraph: “Why is eliminating the use of p-values as a truth arbiter so hard? ‘The basic explanation is neither philosophical nor scientific, but sociologic; everyone uses them,’ says Goodman (2019). ‘It’s the same reason we can use money. When everyone believes in something’s value, we can use it for real things; money for food, and p-values for knowledge claims, publication, funding, and promotion. It doesn’t matter if the p-value doesn’t mean what people think it means; it becomes valuable because of what it buys.’”\nI found this quote striking because it is essentially calling the idea of a p-value a social construct, something that doesn’t actually exist, but is so widely used that no one can stand to get rid of it. I think the comparison to currency is really interesting, that the idea of a p-value is only as important as people make it, the same way that specific pieces of printed paper are also valuable because we believe it so. It highlights how at this point, it doesn’t matter how many people provide good, even great alternatives to the p-value, if they aren’t widely accepted by the statistical community, they have no value. Something needs to be just as convenient, and just as prominent as a p-value to ever take its place, which is incredibly difficult based on the current state of statistical practice."
  },
  {
    "objectID": "posts/03-confidence-intervals/index.html",
    "href": "posts/03-confidence-intervals/index.html",
    "title": "Confidence Intervals",
    "section": "",
    "text": "The confidence intervals we have discussed in class each have associated assumptions in order for us to use them correctly. But, what exactly happens if one of the assumptions is violated? Why is a violated assumption ‘bad’ anyway? In this mini-project, you will investigate what happens to interval width and coverage rate if an assumption is violated for the asymptotic confidence interval for a population proportion.\nThe following project will explore generating confidence intervals of a true proportion over 6 different settings. Having a small, medium, and large \\(n\\), as well as having a true proportion close to 0.5 and far from 0.5.\n\n\n\nlibrary(tidyverse)\n\n\ngenerate_onesamp_cis &lt;- function(n, p, alpha) {\n  \n  ## generate a single sample (one of nsim data sets)\n  x &lt;- rbinom(1, n, p)\n  \n  phat &lt;- x / n\n\n  lb &lt;- phat - qnorm(1 - alpha/2) * sqrt((phat * (1 - phat))/n)\n  ub &lt;- phat + qnorm(1 - alpha/2) * sqrt((phat * (1 - phat))/n)\n  \n  ## put everything into a tibble\n  out_df &lt;- tibble(phat, lb, ub)\n  \n  return(out_df)\n}\n\n# test the function once with random data\ngenerate_onesamp_cis(n = 25, p = 0.5, alpha = 0.1)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.52 0.356 0.684\n\n\n\n\n\n\n## define parameters to use in our function\nn &lt;- 10  # sample size\np &lt;- 0.44    # true proportion\nalpha &lt;- 0.1  # used to construct 90% CI\n\n\\(np \\gt 10 \\rightarrow (10)(0.44) \\gt 10 \\rightarrow 4.4 \\not\\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (50)(0.56) \\gt 10 \\rightarrow 5.6 \\not\\gt 10\\)\nIn this case, the large sample assumption does not hold.\n\nnsim &lt;- 5000  # the number of simulated CIs to create\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n    phat     lb    ub\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1   0.5 0.240  0.760\n 2   0.6 0.345  0.855\n 3   0.5 0.240  0.760\n 4   0.4 0.145  0.655\n 5   0.5 0.240  0.760\n 6   0.3 0.0616 0.538\n 7   0.6 0.345  0.855\n 8   0.3 0.0616 0.538\n 9   0.4 0.145  0.655\n10   0.5 0.240  0.760\n# ℹ 4,990 more rows\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df\n\n# A tibble: 5,000 × 5\n    phat     lb    ub ci_width ci_cover_ind\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n 1   0.5 0.240  0.760    0.520            1\n 2   0.6 0.345  0.855    0.510            1\n 3   0.5 0.240  0.760    0.520            1\n 4   0.4 0.145  0.655    0.510            1\n 5   0.5 0.240  0.760    0.520            1\n 6   0.3 0.0616 0.538    0.477            1\n 7   0.6 0.345  0.855    0.510            1\n 8   0.3 0.0616 0.538    0.477            1\n 9   0.4 0.145  0.655    0.510            1\n10   0.5 0.240  0.760    0.520            1\n# ℹ 4,990 more rows\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.487         0.798\n\n\n\n\n\n\nn &lt;- 50  \np &lt;- 0.44    \n\n\\(np \\gt 10 \\rightarrow (50)(0.44) \\gt 10 \\rightarrow 22 \\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (50)(0.56) \\gt 10 \\rightarrow 28 \\gt 10\\)\nIn this case, the large sample assumption holds.\n\nnsim &lt;- 5000\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.228         0.879\n\n\n\n\n\n\nn &lt;- 1000 \np &lt;- 0.44    \n\n\\(np \\gt 10 \\rightarrow (1000)(0.44) \\gt 10 \\rightarrow 440 \\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (1000)(0.56) \\gt 10 \\rightarrow 560 \\gt 10\\)\nIn this case, the large sample assumption holds.\n\nnsim &lt;- 5000  # the number of simulated CIs to create\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1    0.0516         0.900\n\n\n\n\n\n\nn &lt;- 10 \np &lt;- 0.81   \n\n\\(np \\gt 10 \\rightarrow (10)(0.81) \\gt 10 \\rightarrow 8.1 \\not\\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (10)(0.19) \\gt 10 \\rightarrow 1.9 \\not\\gt 10\\)\nIn this case, the large sample assumption does not hold.\n\nnsim &lt;- 5000 \n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.358         0.851\n\n\n\n\n\n\nn &lt;- 50\np &lt;- 0.81\n\n\\(np \\gt 10 \\rightarrow (50)(0.81) \\gt 10 \\rightarrow 40.5 \\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (50)(0.19) \\gt 10 \\rightarrow 9.5 \\not\\gt 10\\)\nIn this case, the large sample assumption does not hold.\n\nnsim &lt;- 5000  # the number of simulated CIs to create\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.180         0.900\n\n\n\n\n\n\nn &lt;- 1000\np &lt;- 0.81\n\n\\(np \\gt 10 \\rightarrow (1000)(0.81) \\gt 10 \\rightarrow 810 \\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (1000)(0.19) \\gt 10 \\rightarrow 190 \\gt 10\\)\nIn this case, the large sample assumption holds.\n\nnsim &lt;- 5000  # the number of simulated CIs to create\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1    0.0408         0.895\n\n\n\n\n\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 10\\)\n\\(n = 50\\)\n\\(n = 1000\\)\n\n\n\n\n\\(p = 0.44\\)\nCoverage Rate\n0.799\n0.888\n0.893\n\n\n\\(p = 0.81\\)\nCoverage Rate\n0.840\n0.893\n0.903\n\n\n\n\n\n\n\n\n\n\\(p = 0.44\\)\nAverage Width\n0.488\n0.229\n0.0516\n\n\n\\(p = 0.81\\)\nAverage Width\n0.353\n0.179\n0.0408\n\n\n\n\n\n\nFrom class, we saw that the “large sample assumption” only holds when \\(np \\gt 10\\) and \\(n(1-p) \\gt 10\\). We can see from the work here that for both cases were \\(n\\) is small (\\(n = 10\\)) neither proportion value (\\(p = 0.44, 0.81\\)) meets the condition. This indicates that the intervals produced might not be valid. For the medium \\(n\\) values, \\(n = 50\\), the assumption holds when \\(p = 0.44\\), but fails when \\(p = 0.81\\). For large \\(n\\), \\(n = 1000\\), the assumption is met for both values of \\(p\\), meaning that our intervals are likely valid to estimate confidence intervals.\nThe coverage rates for the \\(n = 1000\\) settings are very close to the ideal 90% level, 0.893 and 0.903, for the \\(p = 0.44\\) and \\(p = 0.81\\) settings respectively. This indicates that the use of the normal approximation for the z-score works well in this case. This is consistent with our findings that the “large sample assumption” holds true for large n. For smaller sample sizes (\\(n = 10\\)), the coverage rates are significantly lower for both \\(p\\) settings, which holds with out findings about the “large sample assumption.” Because the assumption was not satisfied, it makes sense that the coverage rate is much lower than the normal approximation would’ve made. For the medium \\(n\\) settings, \\(p = 0.44\\) satisfied the “large sample assumption”, while \\(p = 0.81\\), though the \\(p = 0.81\\) setting still produced a coverage rate closer to 0.9. I would assume that while the setting didn’t satisfy the assumption, \\(n(1-p) = 9.5\\), which might be close enough to 10 that using the normal approximation will still work relatively well.\nWe can see that average width decreases as the sample size increases. For \\(p = 0.44\\), the width drops from 0.488 (\\(n = 10\\)) to 0.0516 (\\(n = 1000\\)). Similarly, for \\(p = 0.81\\), the width decreases from 0.353 (\\(n = 10\\)) to 0.0408 (\\(n = 1000\\)). This makes sense, as increasing the n value will decrease the value of standard error when calculating the value of the upper and lower bounds because the denominator in \\(\\sqrt{\\frac{\\hat{p}(1 - \\hat{p}}{n}}\\). This will decrease the overall average with of the confidence intervals."
  },
  {
    "objectID": "posts/03-confidence-intervals/index.html#loading-packages-and-function-def",
    "href": "posts/03-confidence-intervals/index.html#loading-packages-and-function-def",
    "title": "Confidence Intervals",
    "section": "",
    "text": "library(tidyverse)\n\n\ngenerate_onesamp_cis &lt;- function(n, p, alpha) {\n  \n  ## generate a single sample (one of nsim data sets)\n  x &lt;- rbinom(1, n, p)\n  \n  phat &lt;- x / n\n\n  lb &lt;- phat - qnorm(1 - alpha/2) * sqrt((phat * (1 - phat))/n)\n  ub &lt;- phat + qnorm(1 - alpha/2) * sqrt((phat * (1 - phat))/n)\n  \n  ## put everything into a tibble\n  out_df &lt;- tibble(phat, lb, ub)\n  \n  return(out_df)\n}\n\n# test the function once with random data\ngenerate_onesamp_cis(n = 25, p = 0.5, alpha = 0.1)\n\n# A tibble: 1 × 3\n   phat    lb    ub\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  0.52 0.356 0.684"
  },
  {
    "objectID": "posts/03-confidence-intervals/index.html#setting-1-small-n-proportion-close-to-0.5",
    "href": "posts/03-confidence-intervals/index.html#setting-1-small-n-proportion-close-to-0.5",
    "title": "Confidence Intervals",
    "section": "",
    "text": "## define parameters to use in our function\nn &lt;- 10  # sample size\np &lt;- 0.44    # true proportion\nalpha &lt;- 0.1  # used to construct 90% CI\n\n\\(np \\gt 10 \\rightarrow (10)(0.44) \\gt 10 \\rightarrow 4.4 \\not\\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (50)(0.56) \\gt 10 \\rightarrow 5.6 \\not\\gt 10\\)\nIn this case, the large sample assumption does not hold.\n\nnsim &lt;- 5000  # the number of simulated CIs to create\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\nmany_ci_df\n\n# A tibble: 5,000 × 3\n    phat     lb    ub\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;\n 1   0.5 0.240  0.760\n 2   0.6 0.345  0.855\n 3   0.5 0.240  0.760\n 4   0.4 0.145  0.655\n 5   0.5 0.240  0.760\n 6   0.3 0.0616 0.538\n 7   0.6 0.345  0.855\n 8   0.3 0.0616 0.538\n 9   0.4 0.145  0.655\n10   0.5 0.240  0.760\n# ℹ 4,990 more rows\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df\n\n# A tibble: 5,000 × 5\n    phat     lb    ub ci_width ci_cover_ind\n   &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;        &lt;dbl&gt;\n 1   0.5 0.240  0.760    0.520            1\n 2   0.6 0.345  0.855    0.510            1\n 3   0.5 0.240  0.760    0.520            1\n 4   0.4 0.145  0.655    0.510            1\n 5   0.5 0.240  0.760    0.520            1\n 6   0.3 0.0616 0.538    0.477            1\n 7   0.6 0.345  0.855    0.510            1\n 8   0.3 0.0616 0.538    0.477            1\n 9   0.4 0.145  0.655    0.510            1\n10   0.5 0.240  0.760    0.520            1\n# ℹ 4,990 more rows\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.487         0.798"
  },
  {
    "objectID": "posts/03-confidence-intervals/index.html#setting-2-medium-n-proportion-close-to-0.5",
    "href": "posts/03-confidence-intervals/index.html#setting-2-medium-n-proportion-close-to-0.5",
    "title": "Confidence Intervals",
    "section": "",
    "text": "n &lt;- 50  \np &lt;- 0.44    \n\n\\(np \\gt 10 \\rightarrow (50)(0.44) \\gt 10 \\rightarrow 22 \\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (50)(0.56) \\gt 10 \\rightarrow 28 \\gt 10\\)\nIn this case, the large sample assumption holds.\n\nnsim &lt;- 5000\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.228         0.879"
  },
  {
    "objectID": "posts/03-confidence-intervals/index.html#setting-3-large-n-proportion-close-to-0.5",
    "href": "posts/03-confidence-intervals/index.html#setting-3-large-n-proportion-close-to-0.5",
    "title": "Confidence Intervals",
    "section": "",
    "text": "n &lt;- 1000 \np &lt;- 0.44    \n\n\\(np \\gt 10 \\rightarrow (1000)(0.44) \\gt 10 \\rightarrow 440 \\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (1000)(0.56) \\gt 10 \\rightarrow 560 \\gt 10\\)\nIn this case, the large sample assumption holds.\n\nnsim &lt;- 5000  # the number of simulated CIs to create\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1    0.0516         0.900"
  },
  {
    "objectID": "posts/03-confidence-intervals/index.html#setting-4-small-n-proportion-far-from-0.5",
    "href": "posts/03-confidence-intervals/index.html#setting-4-small-n-proportion-far-from-0.5",
    "title": "Confidence Intervals",
    "section": "",
    "text": "n &lt;- 10 \np &lt;- 0.81   \n\n\\(np \\gt 10 \\rightarrow (10)(0.81) \\gt 10 \\rightarrow 8.1 \\not\\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (10)(0.19) \\gt 10 \\rightarrow 1.9 \\not\\gt 10\\)\nIn this case, the large sample assumption does not hold.\n\nnsim &lt;- 5000 \n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.358         0.851"
  },
  {
    "objectID": "posts/03-confidence-intervals/index.html#setting-5-medium-n-proportion-far-from-0.5",
    "href": "posts/03-confidence-intervals/index.html#setting-5-medium-n-proportion-far-from-0.5",
    "title": "Confidence Intervals",
    "section": "",
    "text": "n &lt;- 50\np &lt;- 0.81\n\n\\(np \\gt 10 \\rightarrow (50)(0.81) \\gt 10 \\rightarrow 40.5 \\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (50)(0.19) \\gt 10 \\rightarrow 9.5 \\not\\gt 10\\)\nIn this case, the large sample assumption does not hold.\n\nnsim &lt;- 5000  # the number of simulated CIs to create\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1     0.180         0.900"
  },
  {
    "objectID": "posts/03-confidence-intervals/index.html#setting-6-large-n-proportion-far-from-0.5",
    "href": "posts/03-confidence-intervals/index.html#setting-6-large-n-proportion-far-from-0.5",
    "title": "Confidence Intervals",
    "section": "",
    "text": "n &lt;- 1000\np &lt;- 0.81\n\n\\(np \\gt 10 \\rightarrow (1000)(0.81) \\gt 10 \\rightarrow 810 \\gt 10\\)\n\\(n(1-p) \\gt \\rightarrow (1000)(0.19) \\gt 10 \\rightarrow 190 \\gt 10\\)\nIn this case, the large sample assumption holds.\n\nnsim &lt;- 5000  # the number of simulated CIs to create\n\nmany_ci_df &lt;- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |&gt;\n  bind_rows()\n\nmany_ci_df &lt;- many_ci_df |&gt; mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p &gt; lb & p &lt; ub,\n                                                          true = 1, \n                                                          false = 0))\n\nmany_ci_df |&gt; summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      &lt;dbl&gt;         &lt;dbl&gt;\n1    0.0408         0.895"
  },
  {
    "objectID": "posts/03-confidence-intervals/index.html#table-of-results",
    "href": "posts/03-confidence-intervals/index.html#table-of-results",
    "title": "Confidence Intervals",
    "section": "",
    "text": "Table of Results\n\n\n\n\n\n\n\n\n\n\n\n\\(n = 10\\)\n\\(n = 50\\)\n\\(n = 1000\\)\n\n\n\n\n\\(p = 0.44\\)\nCoverage Rate\n0.799\n0.888\n0.893\n\n\n\\(p = 0.81\\)\nCoverage Rate\n0.840\n0.893\n0.903\n\n\n\n\n\n\n\n\n\n\\(p = 0.44\\)\nAverage Width\n0.488\n0.229\n0.0516\n\n\n\\(p = 0.81\\)\nAverage Width\n0.353\n0.179\n0.0408"
  },
  {
    "objectID": "posts/03-confidence-intervals/index.html#summary",
    "href": "posts/03-confidence-intervals/index.html#summary",
    "title": "Confidence Intervals",
    "section": "",
    "text": "From class, we saw that the “large sample assumption” only holds when \\(np \\gt 10\\) and \\(n(1-p) \\gt 10\\). We can see from the work here that for both cases were \\(n\\) is small (\\(n = 10\\)) neither proportion value (\\(p = 0.44, 0.81\\)) meets the condition. This indicates that the intervals produced might not be valid. For the medium \\(n\\) values, \\(n = 50\\), the assumption holds when \\(p = 0.44\\), but fails when \\(p = 0.81\\). For large \\(n\\), \\(n = 1000\\), the assumption is met for both values of \\(p\\), meaning that our intervals are likely valid to estimate confidence intervals.\nThe coverage rates for the \\(n = 1000\\) settings are very close to the ideal 90% level, 0.893 and 0.903, for the \\(p = 0.44\\) and \\(p = 0.81\\) settings respectively. This indicates that the use of the normal approximation for the z-score works well in this case. This is consistent with our findings that the “large sample assumption” holds true for large n. For smaller sample sizes (\\(n = 10\\)), the coverage rates are significantly lower for both \\(p\\) settings, which holds with out findings about the “large sample assumption.” Because the assumption was not satisfied, it makes sense that the coverage rate is much lower than the normal approximation would’ve made. For the medium \\(n\\) settings, \\(p = 0.44\\) satisfied the “large sample assumption”, while \\(p = 0.81\\), though the \\(p = 0.81\\) setting still produced a coverage rate closer to 0.9. I would assume that while the setting didn’t satisfy the assumption, \\(n(1-p) = 9.5\\), which might be close enough to 10 that using the normal approximation will still work relatively well.\nWe can see that average width decreases as the sample size increases. For \\(p = 0.44\\), the width drops from 0.488 (\\(n = 10\\)) to 0.0516 (\\(n = 1000\\)). Similarly, for \\(p = 0.81\\), the width decreases from 0.353 (\\(n = 10\\)) to 0.0408 (\\(n = 1000\\)). This makes sense, as increasing the n value will decrease the value of standard error when calculating the value of the upper and lower bounds because the denominator in \\(\\sqrt{\\frac{\\hat{p}(1 - \\hat{p}}{n}}\\). This will decrease the overall average with of the confidence intervals."
  },
  {
    "objectID": "posts/04-bayesian/index.html",
    "href": "posts/04-bayesian/index.html",
    "title": "Bayesian Analysis",
    "section": "",
    "text": "Rafael Nadal is arguably the greatest men’s clay-court tennis player ever to play the game. In this mini-project, you analyze the probability that Nadal wins a point on his own serve against his primary rival, Novak Djokovic, at the French Open (the most prestigious clay court tournament in the world).\n\n\n“I have followed all rules for collaboration for this project, and I have not used generative AI on this project.” Norah Kuduk\n\nlibrary(tidyverse)\nlibrary(knitr)\n\n\n\n\nIn this mini-project we will be researching and analyzing the probability that Nadal wins a point on his own serve against Djokovic on a clay court. To do this, we will use Bayesian models on binomial data (points won). During this project, we will use three different priors, one non-informative, and two informative, one based on a previous match competed by the two players, and one based on a sports commentator’s interpretation of Nadal’s stats.\nAfter creating the three priors, we will update them with the data from Nadal and Djokovic’s 2020 French Open match, where Nadal served 84 points and won 56 of those points. Using the posterior distributions, we can caluclate the posterior mean and credible intervals for each \\(p\\).\n\n\n\n\n\nFor the non-informative prior, we assume that we have no prior information about the possible values of \\(p\\). We can use a distribution of Beta(1,1), which is the same as Unif(0,1), to represent equal likelihood of any probability between 0 and 1.\n\nnoninformative_alpha_prior &lt;- 1\nnoninformative_beta_prior &lt;- 1\n\n\n\n\nAn informative prior based on a clay-court match the two played in the previous year. In that match, Nadal won 46 out of 66 points on his own serve. The standard error of this estimate is 0.05657.\nTo create this informative prior, we can use the fact that in the previous match Nadal won 46 out of 66 points on his serve against Djokovic. Using this information, we can generate a sequence of alphas and use the fact that in a beta distribution \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\) to get a sequence of possible betas. Using those settings, we can calculate the variance of each setting. Whichever setting has the closest variance to \\(0.05657^2\\) is the setting for \\(\\alpha_{prior}\\) and \\(\\beta_{prior}\\).\n\ntarget_mean &lt;- 46 / 66\n\nalphas &lt;- seq(0.01, 100, length.out = 500)\nbetas &lt;- (alphas * (1 - target_mean)) / target_mean\n\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars =\n                                 (alphas * betas) / ((alphas + betas)^2 * (alphas + betas + 1)))\n\ntarget_var &lt;- 0.05657^2\n\nparam_df &lt;- param_df |&gt; mutate(dist_to_target = abs(vars - target_var))\n\nparam_df |&gt; filter(dist_to_target == min(dist_to_target))\n\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1   45.3  19.7 0.00320    0.000000365\n\n\n\nclaycourt_alpha_prior &lt;- 45.3\nclaycourt_beta_prior &lt;- 19.7\n\n\n\n\nCreate an informative prior based on a sports announcer, who claims that they think Nadal wins about 75% of the points on his serve against Djokovic. They are also “almost sure” that Nadal wins no less than 70% of his points on serve against Djokovic.\nTo create this informative prior, we can use the fact that the announcer thinks that wins about 75% of the points on his serve against Djokovic. Using this information, we can generate a sequence of alphas and use the fact that in a beta distribution \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\) to get a sequence of possible betas. Then because the announcer is “almost sure” Nadal wins no less than 70% of his points on serve, we want \\(P(p &lt; 0.7)\\) to be a small value, like 0.02. Then we can go through each alpha and beta setting to find the one that has a probability closest to 0.02. That setting will give us our \\(\\alpha_{prior}\\) and \\(\\beta_{prior}\\).\n\nalphas &lt;- seq(0.1, 300, length.out = 1000) \n\ntarget_mean &lt;- 0.75\ntarget_prob &lt;- 0.02 # almost sure\n\nbetas &lt;- (alphas * (1 - target_mean)) / target_mean\n\nprob_below_70 &lt;- pbeta(0.70, alphas, betas) \n\ntibble(alphas, betas, prob_below_70) |&gt;\n  mutate(close_to_target = abs(prob_below_70 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_below_70 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1   252.  83.9        0.0200       0.0000258\n\n\n\nannouncer_alpha_prior &lt;- 252.1  \nannouncer_beta_prior &lt;- 83.9 \n\n\n\n\n\nlambda_grid &lt;- seq(0, 1, length.out = 1000)\n\nnoninformative_prior &lt;- dbeta(lambda_grid, noninformative_alpha_prior, noninformative_beta_prior)\nclaycourt_prior &lt;- dbeta(lambda_grid, claycourt_alpha_prior, claycourt_beta_prior)\nannouncer_prior &lt;- dbeta(lambda_grid, announcer_alpha_prior, announcer_beta_prior)\n\n\nplot_df &lt;- tibble(lambda_grid, noninformative_prior, claycourt_prior, announcer_prior) |&gt;\n  pivot_longer(cols = -lambda_grid, names_to = \"distribution\", values_to = \"density\") |&gt;\n  separate(distribution, into = c(\"prior_type\", \"distribution\"), sep = \"_\")\n\nggplot(data = plot_df, aes(x = lambda_grid, y = density, color = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(title = \"Prior Distributions\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nx &lt;- 56 # points won by Nadal\nn &lt;- 84 # total points\n\n# Compute posterior parameters\nnoninformative_alpha_post &lt;- noninformative_alpha_prior + x\nnoninformative_beta_post &lt;- noninformative_beta_prior + (n - x)\n\nclaycourt_alpha_post &lt;- claycourt_alpha_prior + x\nclaycourt_beta_post &lt;- claycourt_beta_prior + (n - x)\n\nannouncer_alpha_post &lt;- announcer_alpha_prior + x\nannouncer_beta_post &lt;- announcer_beta_prior + (n - x)\n\n# Compute posterior distributions\nnoninformative_posterior &lt;- dbeta(lambda_grid, noninformative_alpha_post, noninformative_beta_post)\nclaycourt_posterior &lt;- dbeta(lambda_grid, claycourt_alpha_post, claycourt_beta_post)\nannouncer_posterior &lt;- dbeta(lambda_grid, announcer_alpha_post, announcer_beta_post)\n\n\n\n\nplot_df &lt;- tibble(lambda_grid, noninformative_posterior, claycourt_posterior, announcer_posterior) |&gt;\n  pivot_longer(cols = -lambda_grid, names_to = \"distribution\", values_to = \"density\") |&gt;\n  separate(distribution, into = c(\"prior_type\", \"distribution\"), sep = \"_\")\n\nggplot(data = plot_df, aes(x = lambda_grid, y = density, color = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() + \n  labs(title = \"Posterior Distributions\")\n\n\n\n\n\n\n\n\n\n\n\n\nposterior_params &lt;- tibble(\n  model = c(\"Non-Informative\", \"Clay Court\", \"Announcer\"),\n  a_prior = c(noninformative_alpha_prior, claycourt_alpha_prior, announcer_alpha_prior),\n  b_prior = c(noninformative_beta_prior, claycourt_beta_prior, announcer_beta_prior),\n  a_post = c(noninformative_alpha_post, claycourt_alpha_post, announcer_alpha_post),\n  b_post = c(noninformative_beta_post, claycourt_beta_post, announcer_beta_post),\n) |&gt;\n  mutate(\n    posterior_mean = a_post / (a_post + b_post),\n    credible_lower = qbeta(0.05, a_post, b_post),\n    credible_upper = qbeta(0.95, a_post, b_post)\n)\nkable(posterior_params)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\na_prior\nb_prior\na_post\nb_post\nposterior_mean\ncredible_lower\ncredible_upper\n\n\n\n\nNon-Informative\n1.0\n1.0\n57.0\n29.0\n0.6627907\n0.5772453\n0.7440061\n\n\nClay Court\n45.3\n19.7\n101.3\n47.7\n0.6798658\n0.6158332\n0.7411439\n\n\nAnnouncer\n252.1\n83.9\n308.1\n111.9\n0.7335714\n0.6975032\n0.7683736\n\n\n\n\n\n\n\nIn this case, we would likely use the posterior model created from the previous clay court prior, as it balances prior knowledge with observed data. The non-informative prior treats all \\(p\\) values as equally likely, an was heavilily influneced by the observed data. If Nadal is believed to be be the best clay court player, this likely underestimates the \\(p\\) that he wins a point against Djokovic off serve. In addition, the credible interval for the clay court is much narrower than the non-informative model, indicating we are more confident in our estimate. The announcer model has the lowest variance because it’s \\(\\alpha_{prior}\\) and \\(\\beta_{prior}\\) were the significantly larger than the three, meaning that adding the observed data had the least impact, and only served to make the posterior more confident. The downside to this is depending on the credibility of the announcer, his initial belief in Nadal could easily overestimate \\(p\\), meaning that the observed data still doesn’t have as much of an effect.\n\n\n\n\n\nThis mini-project allowed us to experiment with Bayesian models and observe how differing priors can affect the posterior distributions of the data. Priors with large parameters (like the model based of the announcer’s stats) can dominate the results, possibly overlooking the actual affect of the observed data. A non-informative prior relies heavily on the obsserved data, but also can be less confident, resulting in high variance. This is balanced with a model like the model based on a previous match, where the \\(\\alpha_{prior}\\) and \\(\\beta_{prior}\\) provided some information, but was also responsive to new data."
  },
  {
    "objectID": "posts/04-bayesian/index.html#introduction",
    "href": "posts/04-bayesian/index.html#introduction",
    "title": "Bayesian Analysis",
    "section": "",
    "text": "In this mini-project we will be researching and analyzing the probability that Nadal wins a point on his own serve against Djokovic on a clay court. To do this, we will use Bayesian models on binomial data (points won). During this project, we will use three different priors, one non-informative, and two informative, one based on a previous match competed by the two players, and one based on a sports commentator’s interpretation of Nadal’s stats.\nAfter creating the three priors, we will update them with the data from Nadal and Djokovic’s 2020 French Open match, where Nadal served 84 points and won 56 of those points. Using the posterior distributions, we can caluclate the posterior mean and credible intervals for each \\(p\\)."
  },
  {
    "objectID": "posts/04-bayesian/index.html#priors",
    "href": "posts/04-bayesian/index.html#priors",
    "title": "Bayesian Analysis",
    "section": "",
    "text": "For the non-informative prior, we assume that we have no prior information about the possible values of \\(p\\). We can use a distribution of Beta(1,1), which is the same as Unif(0,1), to represent equal likelihood of any probability between 0 and 1.\n\nnoninformative_alpha_prior &lt;- 1\nnoninformative_beta_prior &lt;- 1\n\n\n\n\nAn informative prior based on a clay-court match the two played in the previous year. In that match, Nadal won 46 out of 66 points on his own serve. The standard error of this estimate is 0.05657.\nTo create this informative prior, we can use the fact that in the previous match Nadal won 46 out of 66 points on his serve against Djokovic. Using this information, we can generate a sequence of alphas and use the fact that in a beta distribution \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\) to get a sequence of possible betas. Using those settings, we can calculate the variance of each setting. Whichever setting has the closest variance to \\(0.05657^2\\) is the setting for \\(\\alpha_{prior}\\) and \\(\\beta_{prior}\\).\n\ntarget_mean &lt;- 46 / 66\n\nalphas &lt;- seq(0.01, 100, length.out = 500)\nbetas &lt;- (alphas * (1 - target_mean)) / target_mean\n\nparam_df &lt;- tibble(alphas, betas)\nparam_df &lt;- param_df |&gt; mutate(vars =\n                                 (alphas * betas) / ((alphas + betas)^2 * (alphas + betas + 1)))\n\ntarget_var &lt;- 0.05657^2\n\nparam_df &lt;- param_df |&gt; mutate(dist_to_target = abs(vars - target_var))\n\nparam_df |&gt; filter(dist_to_target == min(dist_to_target))\n\n# A tibble: 1 × 4\n  alphas betas    vars dist_to_target\n   &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;          &lt;dbl&gt;\n1   45.3  19.7 0.00320    0.000000365\n\n\n\nclaycourt_alpha_prior &lt;- 45.3\nclaycourt_beta_prior &lt;- 19.7\n\n\n\n\nCreate an informative prior based on a sports announcer, who claims that they think Nadal wins about 75% of the points on his serve against Djokovic. They are also “almost sure” that Nadal wins no less than 70% of his points on serve against Djokovic.\nTo create this informative prior, we can use the fact that the announcer thinks that wins about 75% of the points on his serve against Djokovic. Using this information, we can generate a sequence of alphas and use the fact that in a beta distribution \\(\\mu = \\frac{\\alpha}{\\alpha + \\beta}\\) to get a sequence of possible betas. Then because the announcer is “almost sure” Nadal wins no less than 70% of his points on serve, we want \\(P(p &lt; 0.7)\\) to be a small value, like 0.02. Then we can go through each alpha and beta setting to find the one that has a probability closest to 0.02. That setting will give us our \\(\\alpha_{prior}\\) and \\(\\beta_{prior}\\).\n\nalphas &lt;- seq(0.1, 300, length.out = 1000) \n\ntarget_mean &lt;- 0.75\ntarget_prob &lt;- 0.02 # almost sure\n\nbetas &lt;- (alphas * (1 - target_mean)) / target_mean\n\nprob_below_70 &lt;- pbeta(0.70, alphas, betas) \n\ntibble(alphas, betas, prob_below_70) |&gt;\n  mutate(close_to_target = abs(prob_below_70 - target_prob)) |&gt;\n  filter(close_to_target == min(close_to_target))\n\n# A tibble: 1 × 4\n  alphas betas prob_below_70 close_to_target\n   &lt;dbl&gt; &lt;dbl&gt;         &lt;dbl&gt;           &lt;dbl&gt;\n1   252.  83.9        0.0200       0.0000258\n\n\n\nannouncer_alpha_prior &lt;- 252.1  \nannouncer_beta_prior &lt;- 83.9 \n\n\n\n\n\nlambda_grid &lt;- seq(0, 1, length.out = 1000)\n\nnoninformative_prior &lt;- dbeta(lambda_grid, noninformative_alpha_prior, noninformative_beta_prior)\nclaycourt_prior &lt;- dbeta(lambda_grid, claycourt_alpha_prior, claycourt_beta_prior)\nannouncer_prior &lt;- dbeta(lambda_grid, announcer_alpha_prior, announcer_beta_prior)\n\n\nplot_df &lt;- tibble(lambda_grid, noninformative_prior, claycourt_prior, announcer_prior) |&gt;\n  pivot_longer(cols = -lambda_grid, names_to = \"distribution\", values_to = \"density\") |&gt;\n  separate(distribution, into = c(\"prior_type\", \"distribution\"), sep = \"_\")\n\nggplot(data = plot_df, aes(x = lambda_grid, y = density, color = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() +\n  labs(title = \"Prior Distributions\")"
  },
  {
    "objectID": "posts/04-bayesian/index.html#posteriors",
    "href": "posts/04-bayesian/index.html#posteriors",
    "title": "Bayesian Analysis",
    "section": "",
    "text": "x &lt;- 56 # points won by Nadal\nn &lt;- 84 # total points\n\n# Compute posterior parameters\nnoninformative_alpha_post &lt;- noninformative_alpha_prior + x\nnoninformative_beta_post &lt;- noninformative_beta_prior + (n - x)\n\nclaycourt_alpha_post &lt;- claycourt_alpha_prior + x\nclaycourt_beta_post &lt;- claycourt_beta_prior + (n - x)\n\nannouncer_alpha_post &lt;- announcer_alpha_prior + x\nannouncer_beta_post &lt;- announcer_beta_prior + (n - x)\n\n# Compute posterior distributions\nnoninformative_posterior &lt;- dbeta(lambda_grid, noninformative_alpha_post, noninformative_beta_post)\nclaycourt_posterior &lt;- dbeta(lambda_grid, claycourt_alpha_post, claycourt_beta_post)\nannouncer_posterior &lt;- dbeta(lambda_grid, announcer_alpha_post, announcer_beta_post)\n\n\n\n\nplot_df &lt;- tibble(lambda_grid, noninformative_posterior, claycourt_posterior, announcer_posterior) |&gt;\n  pivot_longer(cols = -lambda_grid, names_to = \"distribution\", values_to = \"density\") |&gt;\n  separate(distribution, into = c(\"prior_type\", \"distribution\"), sep = \"_\")\n\nggplot(data = plot_df, aes(x = lambda_grid, y = density, color = prior_type)) +\n  geom_line() +\n  scale_colour_viridis_d(end = 0.9) +\n  theme_minimal() + \n  labs(title = \"Posterior Distributions\")\n\n\n\n\n\n\n\n\n\n\n\n\nposterior_params &lt;- tibble(\n  model = c(\"Non-Informative\", \"Clay Court\", \"Announcer\"),\n  a_prior = c(noninformative_alpha_prior, claycourt_alpha_prior, announcer_alpha_prior),\n  b_prior = c(noninformative_beta_prior, claycourt_beta_prior, announcer_beta_prior),\n  a_post = c(noninformative_alpha_post, claycourt_alpha_post, announcer_alpha_post),\n  b_post = c(noninformative_beta_post, claycourt_beta_post, announcer_beta_post),\n) |&gt;\n  mutate(\n    posterior_mean = a_post / (a_post + b_post),\n    credible_lower = qbeta(0.05, a_post, b_post),\n    credible_upper = qbeta(0.95, a_post, b_post)\n)\nkable(posterior_params)\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmodel\na_prior\nb_prior\na_post\nb_post\nposterior_mean\ncredible_lower\ncredible_upper\n\n\n\n\nNon-Informative\n1.0\n1.0\n57.0\n29.0\n0.6627907\n0.5772453\n0.7440061\n\n\nClay Court\n45.3\n19.7\n101.3\n47.7\n0.6798658\n0.6158332\n0.7411439\n\n\nAnnouncer\n252.1\n83.9\n308.1\n111.9\n0.7335714\n0.6975032\n0.7683736\n\n\n\n\n\n\n\nIn this case, we would likely use the posterior model created from the previous clay court prior, as it balances prior knowledge with observed data. The non-informative prior treats all \\(p\\) values as equally likely, an was heavilily influneced by the observed data. If Nadal is believed to be be the best clay court player, this likely underestimates the \\(p\\) that he wins a point against Djokovic off serve. In addition, the credible interval for the clay court is much narrower than the non-informative model, indicating we are more confident in our estimate. The announcer model has the lowest variance because it’s \\(\\alpha_{prior}\\) and \\(\\beta_{prior}\\) were the significantly larger than the three, meaning that adding the observed data had the least impact, and only served to make the posterior more confident. The downside to this is depending on the credibility of the announcer, his initial belief in Nadal could easily overestimate \\(p\\), meaning that the observed data still doesn’t have as much of an effect."
  },
  {
    "objectID": "posts/04-bayesian/index.html#conclusion",
    "href": "posts/04-bayesian/index.html#conclusion",
    "title": "Bayesian Analysis",
    "section": "",
    "text": "This mini-project allowed us to experiment with Bayesian models and observe how differing priors can affect the posterior distributions of the data. Priors with large parameters (like the model based of the announcer’s stats) can dominate the results, possibly overlooking the actual affect of the observed data. A non-informative prior relies heavily on the obsserved data, but also can be less confident, resulting in high variance. This is balanced with a model like the model based on a previous match, where the \\(\\alpha_{prior}\\) and \\(\\beta_{prior}\\) provided some information, but was also responsive to new data."
  },
  {
    "objectID": "posts/01-sampling-distributions/index.html",
    "href": "posts/01-sampling-distributions/index.html",
    "title": "Sampling Distribution",
    "section": "",
    "text": "Carry out simulations of the sampling distributions of the sample minimum (\\(Y_{min}\\)) and the sample maximum (\\(Y_{max}\\)) when taking samples of size \\(n = 5\\) from different populations (specified below). Fill in the summary table in this document and use it answer the questions that follow.\n\n\n“I have followed all rules for collaboration for this project, and I have not used generative AI on this project.”\nNorah Kuduk\n\n\n\n\nlibrary(tidyverse)\nlibrary(gridExtra)\nlibrary(knitr)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\n\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_normal_min &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_normal_mean function\n## nsim times\nnormal_mins &lt;- map_dbl(1:nsim, \\(i) generate_normal_min(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nnormal_mins_df &lt;- tibble(normal_mins)\n\nnormal_min_plot &lt;- ggplot(data = normal_mins_df, aes(x = normal_mins)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Normal Sampling Distribution of the \\nSample Min when n =\", n))\n\nkable(normal_mins_df |&gt;\n  summarise(mean_samp_dist = mean(normal_mins),\n            var_samp_dist = var(normal_mins),\n            se_samp_dist = sd(normal_mins)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n7.662741\n1.816859\n1.347909\n\n\n\n\n\n\n\n\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_normal_max &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000  \n\nnormal_maxs &lt;- map_dbl(1:nsim, \\(i) generate_normal_max(mu = mu, sigma = sigma, n = n))\n\nnormal_maxs_df &lt;- tibble(normal_maxs)\n\nnormal_max_plot &lt;- ggplot(data = normal_maxs_df, aes(x = normal_maxs)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample maxs\",\n       title = paste(\"Normal Sampling Distribution of the \\nSample max when n =\", n))\n\nkable(normal_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(normal_maxs),\n            var_samp_dist = var(normal_maxs),\n            se_samp_dist = sd(normal_maxs)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n12.33985\n1.814831\n1.347157\n\n\n\n\n\n\n\n\n\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\n\n\nn &lt;- 5        \ntheta_1 &lt;- 7         \ntheta_2 &lt;- 13        \n\ngenerate_uniform_min &lt;- function(theta_1, theta_2, n) {\n  \n  single_sample &lt;- runif(n, theta_1, theta_2)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000\n\nuniform_mins &lt;- map_dbl(1:nsim, \\(i) generate_uniform_min(theta_1 = theta_1, theta_2 = theta_2, n = n))\n\nuniform_mins_df &lt;- tibble(uniform_mins)\n\nuniform_min_plot &lt;- ggplot(data = uniform_mins_df, aes(x = uniform_mins)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Uniform Sampling Distribution of the \\nSample Min when n =\", n))\n\nkable(uniform_mins_df |&gt;\n  summarise(mean_samp_dist = mean(uniform_mins),\n            var_samp_dist = var(uniform_mins),\n            se_samp_dist = sd(uniform_mins)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n7.99758\n0.6993106\n0.8362479\n\n\n\n\n\n\n\n\n\nn &lt;- 5    \ntheta_1 &lt;- 7         \ntheta_2 &lt;- 13        \n\ngenerate_uniform_max &lt;- function(theta_1, theta_2, n) {\n  \n  single_sample &lt;- runif(n, theta_1, theta_2)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000\n\nuniform_maxs &lt;- map_dbl(1:nsim, \\(i) generate_uniform_max(theta_1 = theta_1, theta_2 = theta_2, n = n))\n\nuniform_maxs_df &lt;- tibble(uniform_maxs)\n\nuniform_max_plot &lt;- ggplot(data = uniform_maxs_df, aes(x = uniform_maxs)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample maxs\",\n       title = paste(\"Uniform Sampling Distribution of the \\nSample Max when n =\", n))\n\nkable(uniform_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(uniform_maxs),\n            var_samp_dist = var(uniform_maxs),\n            se_samp_dist = sd(uniform_maxs)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n11.97561\n0.7415289\n0.8611207\n\n\n\n\n\n\n\n\n\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\n\n\nn &lt;- 5            \nlambda &lt;- 0.5      \n\ngenerate_exponential_min &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000     \n\nexponential_mins &lt;- map_dbl(1:nsim, \\(i) generate_exponential_min(lambda = lambda, n = n))\n\nexponential_mins_df &lt;- tibble(exponential_mins)\n\nexponential_min_plot &lt;- ggplot(data = exponential_mins_df, aes(x = exponential_mins)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Exponential Sampling Distribution of the \\nSample Min when n =\", n))\n\nkable(exponential_mins_df |&gt;\n  summarise(mean_samp_dist = mean(exponential_mins),\n            var_samp_dist = var(exponential_mins),\n            se_samp_dist = sd(exponential_mins)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n0.399938\n0.1614184\n0.4017691\n\n\n\n\n\n\n\n\n\nn &lt;- 5        \nlambda &lt;- 0.5    \n\ngenerate_exponential_max &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000\n\nexponential_maxs &lt;- map_dbl(1:nsim, \\(i) generate_exponential_max(lambda = lambda, n = n))\n\nexponential_maxs_df &lt;- tibble(exponential_maxs)\n\nexponential_max_plot &lt;- ggplot(data = exponential_maxs_df, aes(x = exponential_maxs)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample maxs\",\n       title = paste(\"Exponential Sampling Distribution of the \\nSample Max when n =\", n))\n\nkable(exponential_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(exponential_maxs),\n            var_samp_dist = var(exponential_maxs),\n            se_samp_dist = sd(exponential_maxs)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n4.540723\n6.007452\n2.45101\n\n\n\n\n\n\n\n\n\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\nn &lt;- 5            # sample size\nalpha &lt;- 8\nbeta &lt;- 2\n\ngenerate_beta_min &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000  \n\nbeta_mins &lt;- map_dbl(1:nsim, \\(i) generate_beta_min(alpha = alpha, beta = beta, n = n))\n\nbeta_mins_df &lt;- tibble(beta_mins)\n\nbeta_min_plot &lt;- ggplot(data = beta_mins_df, aes(x = beta_mins)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Beta Sampling Distribution of the \\nSample Min when n =\", n))\n\nkable(beta_mins_df |&gt;\n  summarise(mean_samp_dist = mean(beta_mins),\n            var_samp_dist = var(beta_mins),\n            se_samp_dist = sd(beta_mins)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n0.643855\n0.0113837\n0.1066944\n\n\n\n\n\n\n\n\n\nn &lt;- 5            # sample size\nalpha &lt;- 8\nbeta &lt;- 2\n\ngenerate_beta_max &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000  \n\nbeta_maxs &lt;- map_dbl(1:nsim, \\(i) generate_beta_max(alpha = alpha, beta = beta, n = n))\n\nbeta_maxs_df &lt;- tibble(beta_maxs)\n\nbeta_max_plot &lt;- ggplot(data = beta_maxs_df, aes(x = beta_maxs)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Beta Sampling Distribution of the \\nSample Max when n =\", n))\n\nkable(beta_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(beta_maxs),\n            var_samp_dist = var(beta_maxs),\n            se_samp_dist = sd(beta_maxs)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n0.9206574\n0.0022239\n0.0471588\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable of Results\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n7.67\n8.00\n0.392\n0.648\n\n\n\\(\\text{E}(Y_{max})\\)\n12.4\n12.0\n4.55\n0.921\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n1.33\n0.862\n0.389\n0.106\n\n\n\\(\\text{SE}(Y_{max})\\)\n1.33\n0.839\n2.43\n0.0459\n\n\n\n\n\n\nFor the Normal population model we can see that the \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) are equal, meaning that the min and max values of a sample have the same (or similar) standard error. For the Uniform model, the \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) are very similar (within 0.35), which also indicates that taking the min or the max has no effect of the variation of the sampling distribution. For the Exponential model, the \\(SE(Y_{min})\\) is much lower than the \\(SE(Y_{max})\\), which indicates that there is less variation in the lowest values of a sample, while there is more variation in the opposite. The reverse is true for the Beta model, as the \\(SE(Y_{min})\\) is much higher than the \\(SE(Y_{max})\\), indicating greater variation in minimum values of a sample.\nFrom these observations, we can use the population distribution model graphs at the top of this file to make some conclusions about how the \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) values compare. For symmetric models like the Normal and the Uniform, there should be little to no variation in the \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) values. However, for skewed models like Exponential and Beta, we should expect a difference. To go further, a right-skewed model (like the Exponential distribution) will have less variation in the minimum, while a left-skewed model (like the Beta model), will have less variation in the maximum.\n\n\n\n\n\nThe pdf of any Exponential population is \\(\\lambda e^{-\\lambda y}\\) for \\(y \\ge 0\\)\nFor \\(Y \\sim  Exp(\\lambda = 0.5)\\)\n\\(f(y) 0.5e^{-0.5y}\\) for \\(y \\ge 0\\)\n\\(F(y) = \\int_{0}^{x} 0.5e^{-0.5y} = 1 - e^{-0.5 y}\\) for \\(y \\ge 0\\)\n\\(f_{min}(y) = n(1-F(y))^{n-1} \\times f(y)\\)\n\\(f_{min}(y) = n(1-(1 - e^{-\\lambda y})^{n-1} \\lambda e^{-\\lambda y}\\)\n\\(f_{min}(y) = n(e^{-\\lambda y})^{n-1}\\lambda e^{-\\lambda y}\\)\n\\(f_{min}(y) = n\\lambda e^{-n\\lambda y + \\lambda y}e^{-\\lambda y}\\)\n\\(f_{min}(y) = n\\lambda e^{-n\\lambda y}\\) where \\(n = 5, \\lambda = 0.5\\)\n\\(f_{min}(y) = 2.5e^{-2.5y}\\) for \\(y \\ge 0\\)\nThis is the same as \\(Y_{min} \\sim Exp(\\lambda = 2.5)\\)\n\\(E(Y_{min}) = \\frac{1}{\\lambda} = \\frac{1}{2.5} = 0.4\\)\n\\(SE(Y_{min}) = \\sqrt{Var(Y_{min})} = \\sqrt{\\frac{1}{\\lambda^2}} = \\sqrt{\\frac{1}{2.5^2}} = 0.4\\)\n\n\n\n\n\n\n\n\n\n\n\n\nThe pdf of any Exponential population is \\(\\lambda e^{-\\lambda y}\\) for \\(y \\ge 0\\)\nFor \\(Y \\sim  Exp(\\lambda = 0.5)\\)\n\\(f(y) 0.5e^{-0.5y}\\) for \\(y \\ge 0\\)\n\\(F(y) = \\int_{0}^{x} 0.5e^{-0.5y} = 1 - e^{-0.5 y}\\) for \\(y \\ge 0\\)\n\\(f_{max}(y) = nF(y)^{n-1} \\times f(y)\\)\n\\(f_{max}(y) = n(1 - e^{-\\lambda y})^{n-1} \\lambda e^{-\\lambda y}\\)\n\\(f_{max}(y) = 2.5(1 - e^{-0.5 y})^{4} e^{-0.5 y}\\) for \\(y \\ge 0\\)\n\\(E(Y_{max}) = \\int_0^\\infty y_{max}2.5(1 - e^{-0.5 y_{max}})^{4} e^{-0.5 y_{max}} = 4.566\\)\n\\(Var(Y_{max}) = E(Y_{max}^2) - [E(Y_{max})]^2\\)\n\\(E(Y_{max}^2) = \\int_0^\\infty y_{max}^2 2.5(1 - e^{-0.5 y_{max}})^{4} e^{-0.5 y_{max}} = 26.709\\)\n\\(Var(Y_{max}) = 26.709 - 4.566^2 = 5.86\\)\n\\(SE(Y_{max}) = \\sqrt{Var(Y_{max})} = \\sqrt{5.86} = 2.42\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulated v. Theoretical Answers\n\n\n\nSimulated\nTheoretical\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n0.392\n0.4\n\n\n\\(\\text{E}(Y_{max})\\)\n4.55\n4.566\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n0.389\n0.4\n\n\n\\(\\text{SE}(Y_{max})\\)\n2.43\n2.42\n\n\n\nWe can see that the simulated and theoretical answers are very close to each other. The \\(E(Y_{min})\\) and \\(E(Y_{max})\\) are almost identical, and the \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) are also very close. This implies that theoretical calculations are correct and that the simulated answers are also correct."
  },
  {
    "objectID": "posts/01-sampling-distributions/index.html#statement-of-integrity",
    "href": "posts/01-sampling-distributions/index.html#statement-of-integrity",
    "title": "Sampling Distribution",
    "section": "",
    "text": "“I have followed all rules for collaboration for this project, and I have not used generative AI on this project.”\nNorah Kuduk"
  },
  {
    "objectID": "posts/01-sampling-distributions/index.html#population-distributions",
    "href": "posts/01-sampling-distributions/index.html#population-distributions",
    "title": "Sampling Distribution",
    "section": "",
    "text": "library(tidyverse)\nlibrary(gridExtra)\nlibrary(knitr)"
  },
  {
    "objectID": "posts/01-sampling-distributions/index.html#normal-distribution",
    "href": "posts/01-sampling-distributions/index.html#normal-distribution",
    "title": "Sampling Distribution",
    "section": "",
    "text": "\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\n\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_normal_min &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000      # number of simulations\n\n## code to map through the function. \n## the \\(i) syntax says to just repeat the generate_normal_mean function\n## nsim times\nnormal_mins &lt;- map_dbl(1:nsim, \\(i) generate_normal_min(mu = mu, sigma = sigma, n = n))\n\n## print some of the 5000 means\n## each number represents the sample mean from __one__ sample.\nnormal_mins_df &lt;- tibble(normal_mins)\n\nnormal_min_plot &lt;- ggplot(data = normal_mins_df, aes(x = normal_mins)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Normal Sampling Distribution of the \\nSample Min when n =\", n))\n\nkable(normal_mins_df |&gt;\n  summarise(mean_samp_dist = mean(normal_mins),\n            var_samp_dist = var(normal_mins),\n            se_samp_dist = sd(normal_mins)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n7.662741\n1.816859\n1.347909\n\n\n\n\n\n\n\n\n\nn &lt;- 5            # sample size\nmu &lt;- 10          # population mean\nsigma &lt;- 2        # population standard deviation\n\ngenerate_normal_max &lt;- function(mu, sigma, n) {\n  \n  single_sample &lt;- rnorm(n, mu, sigma)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000  \n\nnormal_maxs &lt;- map_dbl(1:nsim, \\(i) generate_normal_max(mu = mu, sigma = sigma, n = n))\n\nnormal_maxs_df &lt;- tibble(normal_maxs)\n\nnormal_max_plot &lt;- ggplot(data = normal_maxs_df, aes(x = normal_maxs)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample maxs\",\n       title = paste(\"Normal Sampling Distribution of the \\nSample max when n =\", n))\n\nkable(normal_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(normal_maxs),\n            var_samp_dist = var(normal_maxs),\n            se_samp_dist = sd(normal_maxs)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n12.33985\n1.814831\n1.347157"
  },
  {
    "objectID": "posts/01-sampling-distributions/index.html#uniform-distribution",
    "href": "posts/01-sampling-distributions/index.html#uniform-distribution",
    "title": "Sampling Distribution",
    "section": "",
    "text": "\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\n\n\nn &lt;- 5        \ntheta_1 &lt;- 7         \ntheta_2 &lt;- 13        \n\ngenerate_uniform_min &lt;- function(theta_1, theta_2, n) {\n  \n  single_sample &lt;- runif(n, theta_1, theta_2)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000\n\nuniform_mins &lt;- map_dbl(1:nsim, \\(i) generate_uniform_min(theta_1 = theta_1, theta_2 = theta_2, n = n))\n\nuniform_mins_df &lt;- tibble(uniform_mins)\n\nuniform_min_plot &lt;- ggplot(data = uniform_mins_df, aes(x = uniform_mins)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Uniform Sampling Distribution of the \\nSample Min when n =\", n))\n\nkable(uniform_mins_df |&gt;\n  summarise(mean_samp_dist = mean(uniform_mins),\n            var_samp_dist = var(uniform_mins),\n            se_samp_dist = sd(uniform_mins)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n7.99758\n0.6993106\n0.8362479\n\n\n\n\n\n\n\n\n\nn &lt;- 5    \ntheta_1 &lt;- 7         \ntheta_2 &lt;- 13        \n\ngenerate_uniform_max &lt;- function(theta_1, theta_2, n) {\n  \n  single_sample &lt;- runif(n, theta_1, theta_2)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000\n\nuniform_maxs &lt;- map_dbl(1:nsim, \\(i) generate_uniform_max(theta_1 = theta_1, theta_2 = theta_2, n = n))\n\nuniform_maxs_df &lt;- tibble(uniform_maxs)\n\nuniform_max_plot &lt;- ggplot(data = uniform_maxs_df, aes(x = uniform_maxs)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample maxs\",\n       title = paste(\"Uniform Sampling Distribution of the \\nSample Max when n =\", n))\n\nkable(uniform_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(uniform_maxs),\n            var_samp_dist = var(uniform_maxs),\n            se_samp_dist = sd(uniform_maxs)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n11.97561\n0.7415289\n0.8611207"
  },
  {
    "objectID": "posts/01-sampling-distributions/index.html#exponential-distribution",
    "href": "posts/01-sampling-distributions/index.html#exponential-distribution",
    "title": "Sampling Distribution",
    "section": "",
    "text": "\\(\\text{Exp}(\\lambda = 0.5)\\)\n\n\n\nn &lt;- 5            \nlambda &lt;- 0.5      \n\ngenerate_exponential_min &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000     \n\nexponential_mins &lt;- map_dbl(1:nsim, \\(i) generate_exponential_min(lambda = lambda, n = n))\n\nexponential_mins_df &lt;- tibble(exponential_mins)\n\nexponential_min_plot &lt;- ggplot(data = exponential_mins_df, aes(x = exponential_mins)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Exponential Sampling Distribution of the \\nSample Min when n =\", n))\n\nkable(exponential_mins_df |&gt;\n  summarise(mean_samp_dist = mean(exponential_mins),\n            var_samp_dist = var(exponential_mins),\n            se_samp_dist = sd(exponential_mins)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n0.399938\n0.1614184\n0.4017691\n\n\n\n\n\n\n\n\n\nn &lt;- 5        \nlambda &lt;- 0.5    \n\ngenerate_exponential_max &lt;- function(lambda, n) {\n  \n  single_sample &lt;- rexp(n, lambda)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000\n\nexponential_maxs &lt;- map_dbl(1:nsim, \\(i) generate_exponential_max(lambda = lambda, n = n))\n\nexponential_maxs_df &lt;- tibble(exponential_maxs)\n\nexponential_max_plot &lt;- ggplot(data = exponential_maxs_df, aes(x = exponential_maxs)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample maxs\",\n       title = paste(\"Exponential Sampling Distribution of the \\nSample Max when n =\", n))\n\nkable(exponential_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(exponential_maxs),\n            var_samp_dist = var(exponential_maxs),\n            se_samp_dist = sd(exponential_maxs)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n4.540723\n6.007452\n2.45101"
  },
  {
    "objectID": "posts/01-sampling-distributions/index.html#beta-distribution",
    "href": "posts/01-sampling-distributions/index.html#beta-distribution",
    "title": "Sampling Distribution",
    "section": "",
    "text": "\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\nn &lt;- 5            # sample size\nalpha &lt;- 8\nbeta &lt;- 2\n\ngenerate_beta_min &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_min &lt;- min(single_sample)\n  \n  return(sample_min)\n}\n\nnsim &lt;- 5000  \n\nbeta_mins &lt;- map_dbl(1:nsim, \\(i) generate_beta_min(alpha = alpha, beta = beta, n = n))\n\nbeta_mins_df &lt;- tibble(beta_mins)\n\nbeta_min_plot &lt;- ggplot(data = beta_mins_df, aes(x = beta_mins)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Beta Sampling Distribution of the \\nSample Min when n =\", n))\n\nkable(beta_mins_df |&gt;\n  summarise(mean_samp_dist = mean(beta_mins),\n            var_samp_dist = var(beta_mins),\n            se_samp_dist = sd(beta_mins)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n0.643855\n0.0113837\n0.1066944\n\n\n\n\n\n\n\n\n\nn &lt;- 5            # sample size\nalpha &lt;- 8\nbeta &lt;- 2\n\ngenerate_beta_max &lt;- function(alpha, beta, n) {\n  \n  single_sample &lt;- rbeta(n, alpha, beta)\n  sample_max &lt;- max(single_sample)\n  \n  return(sample_max)\n}\n\nnsim &lt;- 5000  \n\nbeta_maxs &lt;- map_dbl(1:nsim, \\(i) generate_beta_max(alpha = alpha, beta = beta, n = n))\n\nbeta_maxs_df &lt;- tibble(beta_maxs)\n\nbeta_max_plot &lt;- ggplot(data = beta_maxs_df, aes(x = beta_maxs)) +\n  geom_histogram(colour = \"paleturquoise4\", fill = \"paleturquoise2\", bins = 20) +\n  theme_minimal() +\n  labs(x = \"Observed Sample Mins\",\n       title = paste(\"Beta Sampling Distribution of the \\nSample Max when n =\", n))\n\nkable(beta_maxs_df |&gt;\n  summarise(mean_samp_dist = mean(beta_maxs),\n            var_samp_dist = var(beta_maxs),\n            se_samp_dist = sd(beta_maxs)))\n\nWarning in attr(x, \"align\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\nWarning in attr(x, \"format\"): 'xfun::attr()' is deprecated.\nUse 'xfun::attr2()' instead.\nSee help(\"Deprecated\")\n\n\n\n\n\nmean_samp_dist\nvar_samp_dist\nse_samp_dist\n\n\n\n\n0.9206574\n0.0022239\n0.0471588"
  },
  {
    "objectID": "posts/01-sampling-distributions/index.html#table-of-results",
    "href": "posts/01-sampling-distributions/index.html#table-of-results",
    "title": "Sampling Distribution",
    "section": "",
    "text": "Table of Results\n\n\n\n\n\n\n\n\n\n\n\\(\\text{N}(\\mu = 10, \\sigma^2 = 4)\\)\n\\(\\text{Unif}(\\theta_1 = 7, \\theta_2 = 13)\\)\n\\(\\text{Exp}(\\lambda = 0.5)\\)\n\\(\\text{Beta}(\\alpha = 8, \\beta = 2)\\)\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n7.67\n8.00\n0.392\n0.648\n\n\n\\(\\text{E}(Y_{max})\\)\n12.4\n12.0\n4.55\n0.921\n\n\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n1.33\n0.862\n0.389\n0.106\n\n\n\\(\\text{SE}(Y_{max})\\)\n1.33\n0.839\n2.43\n0.0459"
  },
  {
    "objectID": "posts/01-sampling-distributions/index.html#summary",
    "href": "posts/01-sampling-distributions/index.html#summary",
    "title": "Sampling Distribution",
    "section": "",
    "text": "For the Normal population model we can see that the \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) are equal, meaning that the min and max values of a sample have the same (or similar) standard error. For the Uniform model, the \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) are very similar (within 0.35), which also indicates that taking the min or the max has no effect of the variation of the sampling distribution. For the Exponential model, the \\(SE(Y_{min})\\) is much lower than the \\(SE(Y_{max})\\), which indicates that there is less variation in the lowest values of a sample, while there is more variation in the opposite. The reverse is true for the Beta model, as the \\(SE(Y_{min})\\) is much higher than the \\(SE(Y_{max})\\), indicating greater variation in minimum values of a sample.\nFrom these observations, we can use the population distribution model graphs at the top of this file to make some conclusions about how the \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) values compare. For symmetric models like the Normal and the Uniform, there should be little to no variation in the \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) values. However, for skewed models like Exponential and Beta, we should expect a difference. To go further, a right-skewed model (like the Exponential distribution) will have less variation in the minimum, while a left-skewed model (like the Beta model), will have less variation in the maximum."
  },
  {
    "objectID": "posts/01-sampling-distributions/index.html#exponential-population-model",
    "href": "posts/01-sampling-distributions/index.html#exponential-population-model",
    "title": "Sampling Distribution",
    "section": "",
    "text": "The pdf of any Exponential population is \\(\\lambda e^{-\\lambda y}\\) for \\(y \\ge 0\\)\nFor \\(Y \\sim  Exp(\\lambda = 0.5)\\)\n\\(f(y) 0.5e^{-0.5y}\\) for \\(y \\ge 0\\)\n\\(F(y) = \\int_{0}^{x} 0.5e^{-0.5y} = 1 - e^{-0.5 y}\\) for \\(y \\ge 0\\)\n\\(f_{min}(y) = n(1-F(y))^{n-1} \\times f(y)\\)\n\\(f_{min}(y) = n(1-(1 - e^{-\\lambda y})^{n-1} \\lambda e^{-\\lambda y}\\)\n\\(f_{min}(y) = n(e^{-\\lambda y})^{n-1}\\lambda e^{-\\lambda y}\\)\n\\(f_{min}(y) = n\\lambda e^{-n\\lambda y + \\lambda y}e^{-\\lambda y}\\)\n\\(f_{min}(y) = n\\lambda e^{-n\\lambda y}\\) where \\(n = 5, \\lambda = 0.5\\)\n\\(f_{min}(y) = 2.5e^{-2.5y}\\) for \\(y \\ge 0\\)\nThis is the same as \\(Y_{min} \\sim Exp(\\lambda = 2.5)\\)\n\\(E(Y_{min}) = \\frac{1}{\\lambda} = \\frac{1}{2.5} = 0.4\\)\n\\(SE(Y_{min}) = \\sqrt{Var(Y_{min})} = \\sqrt{\\frac{1}{\\lambda^2}} = \\sqrt{\\frac{1}{2.5^2}} = 0.4\\)\n\n\n\n\n\n\n\n\n\n\n\n\nThe pdf of any Exponential population is \\(\\lambda e^{-\\lambda y}\\) for \\(y \\ge 0\\)\nFor \\(Y \\sim  Exp(\\lambda = 0.5)\\)\n\\(f(y) 0.5e^{-0.5y}\\) for \\(y \\ge 0\\)\n\\(F(y) = \\int_{0}^{x} 0.5e^{-0.5y} = 1 - e^{-0.5 y}\\) for \\(y \\ge 0\\)\n\\(f_{max}(y) = nF(y)^{n-1} \\times f(y)\\)\n\\(f_{max}(y) = n(1 - e^{-\\lambda y})^{n-1} \\lambda e^{-\\lambda y}\\)\n\\(f_{max}(y) = 2.5(1 - e^{-0.5 y})^{4} e^{-0.5 y}\\) for \\(y \\ge 0\\)\n\\(E(Y_{max}) = \\int_0^\\infty y_{max}2.5(1 - e^{-0.5 y_{max}})^{4} e^{-0.5 y_{max}} = 4.566\\)\n\\(Var(Y_{max}) = E(Y_{max}^2) - [E(Y_{max})]^2\\)\n\\(E(Y_{max}^2) = \\int_0^\\infty y_{max}^2 2.5(1 - e^{-0.5 y_{max}})^{4} e^{-0.5 y_{max}} = 26.709\\)\n\\(Var(Y_{max}) = 26.709 - 4.566^2 = 5.86\\)\n\\(SE(Y_{max}) = \\sqrt{Var(Y_{max})} = \\sqrt{5.86} = 2.42\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nSimulated v. Theoretical Answers\n\n\n\nSimulated\nTheoretical\n\n\n\n\n\\(\\text{E}(Y_{min})\\)\n0.392\n0.4\n\n\n\\(\\text{E}(Y_{max})\\)\n4.55\n4.566\n\n\n\n\n\n\n\n\\(\\text{SE}(Y_{min})\\)\n0.389\n0.4\n\n\n\\(\\text{SE}(Y_{max})\\)\n2.43\n2.42\n\n\n\nWe can see that the simulated and theoretical answers are very close to each other. The \\(E(Y_{min})\\) and \\(E(Y_{max})\\) are almost identical, and the \\(SE(Y_{min})\\) and \\(SE(Y_{max})\\) are also very close. This implies that theoretical calculations are correct and that the simulated answers are also correct."
  },
  {
    "objectID": "posts/02-estimation/index.html",
    "href": "posts/02-estimation/index.html",
    "title": "Estimation",
    "section": "",
    "text": "For this mini-project, you will write a “meaningful story.” A “meaningful story” is one continuous piece of writing / creative work that uses key words from a list and in which the sentences “make sense and hang together.” That is, the ideas in the story must illustrate that you understand key concepts from Stat 326 in a way that allows you to write “meaningfully” about them.\n\n\nThe St. Lawrence University swim team is travelling to their Liberty League championships next week, but their coach is still trying to figure out how to organize the final relay. Normally, the 400 yard free relay has the top 4 fastest swimmers in the event compete in the A relay, the next 4 in the B, and so on. This year, the 4th and 5th place swimmers have 100 yard free times within one-tenth of each other, and almost identical relay splits as well. The coach has to figure out who is going to go in which relay. Instead of randomly picking one or the other, the coach hears that statistics might be able to help them make a fair decision for both swimmers. Ideally, the swimmer best fit for the relay would go just over 54 seconds, which meant that the coach needed to figure out who was most likely to go that time using results from the rest of the season.\nThe average time of a swimmer’s 100 yard freestyle over the course of a season is a parameter, which is an unknown value that can hopefully give an idea of the swimmer’s performance. Because the coach was out of time, and championships was this week, they needed to use an estimator, a function of sample data that could provide an approximation of average time. Using data from all the 100 freestyles the swimmers competed in over the course of the season, they took a random sample of 5 races and calculated both swimmer’s average split time. This sample statistic served as an estimate of the true parameter.\nThe time of each 100 is a random variable, which means that the coach knows that there will be some variation across times, as no one can go the same time over and over and over again. The coach assumed that each swimmer’s 100 times followed a Normal distribution where times exist around an average value with some variation in them.\nBefore creating an estimator and finding an estimate, the coach has to consider possible bias. In an ideal world, the swimmers could swim at their best and their fastest, every single time they swam. This obviously isn’t accurate, so only taking the meets where one swimmer was rested before the race, and the other was sick, would overestimate and underestimate the times respectively. To reduce the bias in the sample, the coach made sure times were picked randomly to get a better overall view. The coach also knows that there can be variance across samples, so they checked whether increasing the sample size to more than 5 races would reduce variability. In addition, when picking the estimator the coach needs to ensure that is consistent, so that as sample size would increase, the estimate would converge to the true value of the population.\nUsing this, the coach analyzed the likelihood of each swimmer going the ideal time by considering samples of their times throughout the season. Each time, the coach calculated the probability that the average time was below 54 seconds. Even with statistical methods, the competition was close, and both swimmers had a high likelihood of being successful. Still the coach put swimmer 1 into the A relay, and swimmer 2 into the B, with the explanation of “it’s what the math says.” In their respective relays, both swimmers swim great, though swimmer 1 goes slightly faster, and the coach breathes a small sigh of relief that the numbers didn’t fail her this time. There’s always next time though…"
  },
  {
    "objectID": "posts/02-estimation/index.html#the-relay-conundrum",
    "href": "posts/02-estimation/index.html#the-relay-conundrum",
    "title": "Estimation",
    "section": "",
    "text": "The St. Lawrence University swim team is travelling to their Liberty League championships next week, but their coach is still trying to figure out how to organize the final relay. Normally, the 400 yard free relay has the top 4 fastest swimmers in the event compete in the A relay, the next 4 in the B, and so on. This year, the 4th and 5th place swimmers have 100 yard free times within one-tenth of each other, and almost identical relay splits as well. The coach has to figure out who is going to go in which relay. Instead of randomly picking one or the other, the coach hears that statistics might be able to help them make a fair decision for both swimmers. Ideally, the swimmer best fit for the relay would go just over 54 seconds, which meant that the coach needed to figure out who was most likely to go that time using results from the rest of the season.\nThe average time of a swimmer’s 100 yard freestyle over the course of a season is a parameter, which is an unknown value that can hopefully give an idea of the swimmer’s performance. Because the coach was out of time, and championships was this week, they needed to use an estimator, a function of sample data that could provide an approximation of average time. Using data from all the 100 freestyles the swimmers competed in over the course of the season, they took a random sample of 5 races and calculated both swimmer’s average split time. This sample statistic served as an estimate of the true parameter.\nThe time of each 100 is a random variable, which means that the coach knows that there will be some variation across times, as no one can go the same time over and over and over again. The coach assumed that each swimmer’s 100 times followed a Normal distribution where times exist around an average value with some variation in them.\nBefore creating an estimator and finding an estimate, the coach has to consider possible bias. In an ideal world, the swimmers could swim at their best and their fastest, every single time they swam. This obviously isn’t accurate, so only taking the meets where one swimmer was rested before the race, and the other was sick, would overestimate and underestimate the times respectively. To reduce the bias in the sample, the coach made sure times were picked randomly to get a better overall view. The coach also knows that there can be variance across samples, so they checked whether increasing the sample size to more than 5 races would reduce variability. In addition, when picking the estimator the coach needs to ensure that is consistent, so that as sample size would increase, the estimate would converge to the true value of the population.\nUsing this, the coach analyzed the likelihood of each swimmer going the ideal time by considering samples of their times throughout the season. Each time, the coach calculated the probability that the average time was below 54 seconds. Even with statistical methods, the competition was close, and both swimmers had a high likelihood of being successful. Still the coach put swimmer 1 into the A relay, and swimmer 2 into the B, with the explanation of “it’s what the math says.” In their respective relays, both swimmers swim great, though swimmer 1 goes slightly faster, and the coach breathes a small sigh of relief that the numbers didn’t fail her this time. There’s always next time though…"
  },
  {
    "objectID": "posts/06-miniproject-reflection/index.html",
    "href": "posts/06-miniproject-reflection/index.html",
    "title": "Mini-Project Refection",
    "section": "",
    "text": "The first mini-project we did involved simulations of the sampling distributions of the sample minimum (\\(Y_{min}\\)) and the sample maximum (\\(Y_{max}\\)) when taking samples from different populations (specified below). This connected to the overall goal of section 1, where our goals involved constructing the sampling distribution of any sample statistic (sample mean, sample median, sample maximum, etc.) given a population model for the simulation, explaining how a change in sample size affects the center and spread of a sampling distribution, and matching properties of the simulated sampling distribution of the sample mean to the theoretical sampling distribution of the sample mean that we did in probability. The second mini-project involved writing a story that demonstrated our understanding of terms related to the estimation unit of class. In the story, we had to highlight ideas like bias, variance, estimation, and sampling distributions, and how they connected to one another in a real-life/physical scenario. The third miniproject was based on what happens to interval width and coverage rate if an assumption is violated for the asymptotic confidence interval for a population proportion. This expanded on what we had done in class, as all of the examples we had passed the assumptions, meaning all intervals and coverage rates found were likely valid to estimate confidence intervals. Without having those assumptions, we cannot say the same. This connected directly to Central Limit Theorem and sampling variability.\nThe fourth miniproject was a transition from a frequentist model to a Bayesian model of thought. In the miniproject, we were able to compare informative and non-informative priors, showing how prior beliefs influence posterior estimates. This allowed us to see how Bayesian inference can be used to update our beliefs about a parameter based on new data, which is a key concept in statistics. The fifth miniproject went back to the frequentist model of statistics, and involved examining the use and limitations of p-values and their current significance in statistics. It succeeded in connecting significance theory to real-world statistical ethics and communication, and also allowed us to think about the implications of the work we when using p-values for hypothesis testing. As a result, we were able to formulate opinions on the the current statistical significance versus practical significance of p-values, and how we might adjust the perspective of the statistical community as a whole.\nThinking about how miniprojects connected to eachother, miniprojects 1, 3, and 4 all required us to simulate during the assignment, which highlighted how improtant approixmation is to statistics and how much we rely on it. Instead of simulating a statistical scenario, miniprojects 2 and 5 were examples of how we might need to communicate our understanding of statistical concepts through words and explanation, rather than math. These two projects emphasized the importance of clear communication in statistics, as it is not just about the math, but also about conveying the reasoning and implications behind the numbers. Miniprojects 3 and 5 also connected through talking about confidence and uncertainty in statistics. With miniproject 3 highlighting confidence through intervals, while miniproject 5 highlighted thr broader implications of quantifying uncertainty with the current standards. Finally we can think about how miniproject 4 offered a Bayesian contrast to the frequentist ideas explored in miniprojects 3 (CI coverage) and 5 (p-values).\nOverall, the miniprojects in this course were essential in helping us to learn statistical concepts in a way that was different to a homework or graded in class assessment. The miniprojects allowed us to grasp key concepts through practical applications. Each of the five sections of the course had an associated miniproject that allowed us to build on our understanding from the unit, as well as reflect on connections from other sections not directly connceted. From miniproject 1, we can take away that statistical theory and simulation work together. From miniprojects 2 and 5, we can conclude that clear statistical communication is vital to the study of statistics, both for understanding and later research. From miniprojects 3 and 4, we can think about how Bayesian statistics can provide an alternate approach to uncertainty, contrasting it with the use of uncertainty in the frequentist perspective. Both are good, but each has diffreent benefits and drawbacks to their use."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STAT326 Blog",
    "section": "",
    "text": "Mini-Project Refection\n\n\n\n\n\n\nreflection\n\n\n\n\n\n\n\n\n\nApr 28, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nHypothesis Testing\n\n\n\n\n\n\nmini-project\n\n\n\n\n\n\n\n\n\nApr 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nBayesian Analysis\n\n\n\n\n\n\nmini-project\n\n\n\n\n\n\n\n\n\nApr 2, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nConfidence Intervals\n\n\n\n\n\n\nmini-project\n\n\n\n\n\n\n\n\n\nMar 5, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nEstimation\n\n\n\n\n\n\nmini-project\n\n\n\n\n\n\n\n\n\nFeb 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nSampling Distribution\n\n\n\n\n\n\nmini-project\n\n\n\n\n\n\n\n\n\nJan 29, 2025\n\n\n\n\n\n\nNo matching items"
  }
]