{
  "hash": "97c57bbf0a0c4dab988ae9594155a0f5",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Confidence Intervals\"\ndate: \"03-05-2025\"\nformat: html\ncategories:\n  - mini-project\n---\n\n\n# Mini Project 3: Simulation to Investigate Confidence Intervals\n\nThe confidence intervals we have discussed in class each have associated assumptions in order for us to use them correctly. But, what exactly happens if one of the assumptions is violated? Why is a violated assumption ‘bad’ anyway? In this mini-project, you will investigate what happens to interval width and coverage rate if an assumption is violated for the asymptotic confidence interval for a population proportion.\n\n\nThe following project will explore generating confidence intervals of a true proportion over 6 different settings. Having a small, medium, and large $n$, as well as having a true proportion close to 0.5 and far from 0.5.\n\n## Loading Packages and Function Def\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\ngenerate_onesamp_cis <- function(n, p, alpha) {\n  \n  ## generate a single sample (one of nsim data sets)\n  x <- rbinom(1, n, p)\n  \n  phat <- x / n\n\n  lb <- phat - qnorm(1 - alpha/2) * sqrt((phat * (1 - phat))/n)\n  ub <- phat + qnorm(1 - alpha/2) * sqrt((phat * (1 - phat))/n)\n  \n  ## put everything into a tibble\n  out_df <- tibble(phat, lb, ub)\n  \n  return(out_df)\n}\n\n# test the function once with random data\ngenerate_onesamp_cis(n = 25, p = 0.5, alpha = 0.1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 3\n   phat    lb    ub\n  <dbl> <dbl> <dbl>\n1  0.52 0.356 0.684\n```\n\n\n:::\n:::\n\n\n\n## Setting 1: Small n, Proportion close to 0.5\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n## define parameters to use in our function\nn <- 10  # sample size\np <- 0.44    # true proportion\nalpha <- 0.1  # used to construct 90% CI\n```\n:::\n\n\n\n$np \\gt 10 \\rightarrow (10)(0.44) \\gt 10 \\rightarrow 4.4 \\not\\gt 10$\n\n$n(1-p) \\gt \\rightarrow (50)(0.56) \\gt 10 \\rightarrow 5.6 \\not\\gt 10$\n\nIn this case, the large sample assumption does not hold. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsim <- 5000  # the number of simulated CIs to create\n\nmany_ci_df <- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |>\n  bind_rows()\nmany_ci_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5,000 × 3\n    phat       lb    ub\n   <dbl>    <dbl> <dbl>\n 1   0.5  0.240   0.760\n 2   0.4  0.145   0.655\n 3   0.4  0.145   0.655\n 4   0.3  0.0616  0.538\n 5   0.6  0.345   0.855\n 6   0.2 -0.00806 0.408\n 7   0.3  0.0616  0.538\n 8   0.8  0.592   1.01 \n 9   0.5  0.240   0.760\n10   0.5  0.240   0.760\n# ℹ 4,990 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nmany_ci_df <- many_ci_df |> mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p > lb & p < ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 5,000 × 5\n    phat       lb    ub ci_width ci_cover_ind\n   <dbl>    <dbl> <dbl>    <dbl>        <dbl>\n 1   0.5  0.240   0.760    0.520            1\n 2   0.4  0.145   0.655    0.510            1\n 3   0.4  0.145   0.655    0.510            1\n 4   0.3  0.0616  0.538    0.477            1\n 5   0.6  0.345   0.855    0.510            1\n 6   0.2 -0.00806 0.408    0.416            0\n 7   0.3  0.0616  0.538    0.477            1\n 8   0.8  0.592   1.01     0.416            0\n 9   0.5  0.240   0.760    0.520            1\n10   0.5  0.240   0.760    0.520            1\n# ℹ 4,990 more rows\n```\n\n\n:::\n\n```{.r .cell-code}\nmany_ci_df |> summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      <dbl>         <dbl>\n1     0.486         0.798\n```\n\n\n:::\n:::\n\n\n\n## Setting 2: Medium n, Proportion close to 0.5\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 50  \np <- 0.44    \n```\n:::\n\n\n\n$np \\gt 10 \\rightarrow (50)(0.44) \\gt 10 \\rightarrow 22 \\gt 10$\n\n$n(1-p) \\gt \\rightarrow (50)(0.56) \\gt 10 \\rightarrow 28 \\gt 10$\n\nIn this case, the large sample assumption holds. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsim <- 5000\n\nmany_ci_df <- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |>\n  bind_rows()\n\nmany_ci_df <- many_ci_df |> mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p > lb & p < ub,\n                                                          true = 1, \n                                                          false = 0))\n\nmany_ci_df |> summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      <dbl>         <dbl>\n1     0.229         0.885\n```\n\n\n:::\n:::\n\n\n\n## Setting 3: Large n, Proportion close to 0.5\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000 \np <- 0.44    \n```\n:::\n\n\n\n$np \\gt 10 \\rightarrow (1000)(0.44) \\gt 10 \\rightarrow 440 \\gt 10$\n\n$n(1-p) \\gt \\rightarrow (1000)(0.56) \\gt 10 \\rightarrow 560 \\gt 10$\n\nIn this case, the large sample assumption holds. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsim <- 5000  # the number of simulated CIs to create\n\nmany_ci_df <- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |>\n  bind_rows()\n\nmany_ci_df <- many_ci_df |> mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p > lb & p < ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |> summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      <dbl>         <dbl>\n1    0.0516         0.896\n```\n\n\n:::\n:::\n\n\n\n## Setting 4: Small n, Proportion far from 0.5\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 10 \np <- 0.81   \n```\n:::\n\n\n\n$np \\gt 10 \\rightarrow (10)(0.81) \\gt 10 \\rightarrow 8.1 \\not\\gt 10$\n\n$n(1-p) \\gt \\rightarrow (10)(0.19) \\gt 10 \\rightarrow 1.9 \\not\\gt 10$\n\nIn this case, the large sample assumption does not hold.  \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsim <- 5000 \n\nmany_ci_df <- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |>\n  bind_rows()\n\nmany_ci_df <- many_ci_df |> mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p > lb & p < ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |> summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      <dbl>         <dbl>\n1     0.356         0.846\n```\n\n\n:::\n:::\n\n\n\n## Setting 5: Medium n, Proportion far from 0.5\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 50\np <- 0.81\n```\n:::\n\n\n\n$np \\gt 10 \\rightarrow (50)(0.81) \\gt 10 \\rightarrow 40.5 \\gt 10$\n\n$n(1-p) \\gt \\rightarrow (50)(0.19) \\gt 10 \\rightarrow 9.5 \\not\\gt 10$\n\nIn this case, the large sample assumption does not hold. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsim <- 5000  # the number of simulated CIs to create\n\nmany_ci_df <- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |>\n  bind_rows()\n\nmany_ci_df <- many_ci_df |> mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p > lb & p < ub,\n                                                          true = 1, \n                                                          false = 0))\nmany_ci_df |> summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      <dbl>         <dbl>\n1     0.180         0.890\n```\n\n\n:::\n:::\n\n\n\n## Setting 6: Large n, Proportion far from 0.5\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nn <- 1000\np <- 0.81\n```\n:::\n\n\n\n$np \\gt 10 \\rightarrow (1000)(0.81) \\gt 10 \\rightarrow 810 \\gt 10$\n\n$n(1-p) \\gt \\rightarrow (1000)(0.19) \\gt 10 \\rightarrow 190 \\gt 10$\n\nIn this case, the large sample assumption holds. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnsim <- 5000  # the number of simulated CIs to create\n\nmany_ci_df <- map(1:nsim,\n                  \\(i) generate_onesamp_cis(n = n, p = p,\n                                            alpha = alpha)) |>\n  bind_rows()\n\nmany_ci_df <- many_ci_df |> mutate(ci_width = ub - lb,\n                                   ci_cover_ind = if_else(p > lb & p < ub,\n                                                          true = 1, \n                                                          false = 0))\n\nmany_ci_df |> summarise(avg_width = mean(ci_width),\n                        coverage_rate = mean(ci_cover_ind))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  avg_width coverage_rate\n      <dbl>         <dbl>\n1    0.0408         0.901\n```\n\n\n:::\n:::\n\n\n\n## Table of Results\n\n|  |         | $n = 10$ | $n = 50$ | $n = 1000$ |\n|:----:|:-----------------:|:-------------:|:------------:|:------------:|\n| $p = 0.44$   | Coverage Rate       | 0.799  | 0.888  | 0.893  |\n| $p = 0.81$   | Coverage Rate       | 0.840  | 0.893  | 0.903  |\n|    |                     |               |              |              |\n| $p = 0.44$    | Average Width        | 0.488  | 0.229  | 0.0516  |\n| $p = 0.81$    | Average Width        | 0.353  | 0.179  | 0.0408  |\n\n: Table of Results {.striped .hover}\n\n## Summary\n\nFrom class, we saw that the “large sample assumption” only holds when $np \\gt 10$ and $n(1-p) \\gt 10$. We can see from the work here that for both cases were $n$ is small ($n = 10$) neither proportion value ($p = 0.44, 0.81$) meets the condition. This indicates that the intervals produced might not be valid. For the medium $n$ values, $n = 50$, the assumption holds when $p = 0.44$, but fails when $p = 0.81$. For large $n$, $n = 1000$, the assumption is met for both values of $p$, meaning that our intervals are likely valid to estimate confidence intervals. \n\nThe coverage rates for the $n = 1000$ settings are very close to the ideal 90% level, 0.893 and 0.903, for the $p = 0.44$ and $p = 0.81$ settings respectively. This indicates that the use of the normal approximation for the z-score works well in this case. This is consistent with our findings that the “large sample assumption” holds true for large n. For smaller sample sizes ($n = 10$), the coverage rates are significantly lower for both $p$ settings, which holds with out findings about the \"large sample assumption.\" Because the assumption was not satisfied, it makes sense that the coverage rate is much lower than the normal approximation would've made. For the medium $n$ settings, $p = 0.44$ satisfied the \"large sample assumption\", while $p = 0.81$, though the $p = 0.81$ setting still produced a coverage rate closer to 0.9. I would assume that while the setting didn't satisfy the assumption, $n(1-p) = 9.5$, which might be close enough to 10 that using the normal approximation will still work relatively well. \n\nWe can see that average width decreases as the sample size increases. For $p = 0.44$, the width drops from 0.488 ($n = 10$) to 0.0516 ($n = 1000$). Similarly, for $p = 0.81$, the width decreases from 0.353 ($n = 10$) to 0.0408 ($n = 1000$). This makes sense, as increasing the n value will decrease the value of standard error when calculating the value of the upper and lower bounds because the denominator in $\\sqrt{\\frac{\\hat{p}(1 - \\hat{p}}{n}}$. This will decrease the overall average with of the confidence intervals. \n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}